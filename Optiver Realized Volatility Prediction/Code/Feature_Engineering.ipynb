{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b6af65",
   "metadata": {
    "papermill": {
     "duration": 0.017995,
     "end_time": "2025-04-19T09:12:52.158043",
     "exception": false,
     "start_time": "2025-04-19T09:12:52.140048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Optiver Realized Volatility Predictions\n",
    "> ⚠️ **Note**  \n",
    "Due to the large size of the dataset, raw data files are not uploaded to this repository.  \n",
    "You can access the original data from the official Kaggle competition page:  [Optiver Realized Volatility Prediction](https://www.kaggle.com/competitions/optiver-realized-volatility-prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e88a5",
   "metadata": {
    "papermill": {
     "duration": 0.015951,
     "end_time": "2025-04-19T09:12:52.190511",
     "exception": false,
     "start_time": "2025-04-19T09:12:52.174560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6557bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:12:52.233625Z",
     "iopub.status.busy": "2025-04-19T09:12:52.232965Z",
     "iopub.status.idle": "2025-04-19T09:13:30.757652Z",
     "shell.execute_reply": "2025-04-19T09:13:30.757098Z",
     "shell.execute_reply.started": "2025-04-13T18:10:19.424939Z"
    },
    "papermill": {
     "duration": 38.550974,
     "end_time": "2025-04-19T09:13:30.757793",
     "exception": false,
     "start_time": "2025-04-19T09:12:52.206819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f9cf95",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-19T09:13:30.800477Z",
     "iopub.status.busy": "2025-04-19T09:13:30.799989Z",
     "iopub.status.idle": "2025-04-19T09:13:33.338372Z",
     "shell.execute_reply": "2025-04-19T09:13:33.338826Z",
     "shell.execute_reply.started": "2025-04-13T18:11:16.911816Z"
    },
    "papermill": {
     "duration": 2.563831,
     "end_time": "2025-04-19T09:13:33.338986",
     "exception": false,
     "start_time": "2025-04-19T09:13:30.775155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "\n",
    "# setting some globl config\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "orange_black = [\n",
    "    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n",
    "]\n",
    "plt.rcParams['figure.figsize'] = (16,9)\n",
    "plt.rcParams[\"figure.facecolor\"] = '#FFFACD'\n",
    "plt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"grid.color\"] = orange_black[3]\n",
    "plt.rcParams[\"grid.alpha\"] = 0.5\n",
    "plt.rcParams[\"grid.linestyle\"] = '--'\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    import os, random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    try:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55826adb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:13:33.378335Z",
     "iopub.status.busy": "2025-04-19T09:13:33.377655Z",
     "iopub.status.idle": "2025-04-19T09:13:33.380702Z",
     "shell.execute_reply": "2025-04-19T09:13:33.381098Z",
     "shell.execute_reply.started": "2025-04-04T05:53:57.649056Z"
    },
    "papermill": {
     "duration": 0.025303,
     "end_time": "2025-04-19T09:13:33.381223",
     "exception": false,
     "start_time": "2025-04-19T09:13:33.355920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47985bcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:13:33.419418Z",
     "iopub.status.busy": "2025-04-19T09:13:33.418949Z",
     "iopub.status.idle": "2025-04-19T09:13:33.488284Z",
     "shell.execute_reply": "2025-04-19T09:13:33.488793Z",
     "shell.execute_reply.started": "2025-04-04T05:53:57.656694Z"
    },
    "papermill": {
     "duration": 0.090213,
     "end_time": "2025-04-19T09:13:33.488970",
     "exception": false,
     "start_time": "2025-04-19T09:13:33.398757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 19 09:13:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P0             25W /  250W |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b80a0",
   "metadata": {
    "papermill": {
     "duration": 0.016749,
     "end_time": "2025-04-19T09:13:33.522940",
     "exception": false,
     "start_time": "2025-04-19T09:13:33.506191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a06e225f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:13:33.562796Z",
     "iopub.status.busy": "2025-04-19T09:13:33.562223Z",
     "iopub.status.idle": "2025-04-19T09:13:33.564763Z",
     "shell.execute_reply": "2025-04-19T09:13:33.564321Z",
     "shell.execute_reply.started": "2025-04-13T18:11:32.617536Z"
    },
    "papermill": {
     "duration": 0.02516,
     "end_time": "2025-04-19T09:13:33.564884",
     "exception": false,
     "start_time": "2025-04-19T09:13:33.539724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_train_test():\n",
    "    # Function to read our base train and test set\n",
    "    \n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    print(f'Our test set has {test.shape[0]} rows')\n",
    "    print(f'Our training set has {train.isna().sum().sum()} missing values')\n",
    "    print(f'Our test set has {test.isna().sum().sum()} missing values')\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86b6181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:13:33.602794Z",
     "iopub.status.busy": "2025-04-19T09:13:33.602237Z",
     "iopub.status.idle": "2025-04-19T09:13:34.525635Z",
     "shell.execute_reply": "2025-04-19T09:13:34.525996Z",
     "shell.execute_reply.started": "2025-04-13T18:11:35.556097Z"
    },
    "papermill": {
     "duration": 0.944301,
     "end_time": "2025-04-19T09:13:34.526146",
     "exception": false,
     "start_time": "2025-04-19T09:13:33.581845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n",
      "Our test set has 3 rows\n",
      "Our training set has 0 missing values\n",
      "Our test set has 0 missing values\n"
     ]
    }
   ],
   "source": [
    "train, test = read_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ec192",
   "metadata": {
    "papermill": {
     "duration": 0.016862,
     "end_time": "2025-04-19T09:13:34.560304",
     "exception": false,
     "start_time": "2025-04-19T09:13:34.543442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7b25ce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:13:34.635613Z",
     "iopub.status.busy": "2025-04-19T09:13:34.613204Z",
     "iopub.status.idle": "2025-04-19T09:13:34.637731Z",
     "shell.execute_reply": "2025-04-19T09:13:34.638089Z",
     "shell.execute_reply.started": "2025-04-13T18:11:46.442228Z"
    },
    "papermill": {
     "duration": 0.060785,
     "end_time": "2025-04-19T09:13:34.638215",
     "exception": false,
     "start_time": "2025-04-19T09:13:34.577430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "def calc_wap1(df):\n",
    "    # Function to calculate first WAP\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap2(df):\n",
    "    # Function to calculate second WAP\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def log_return(series):\n",
    "    # Function to calculate the log of the return\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def realized_volatility(series):\n",
    "    # Calculate the realized volatility\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "def count_unique(series):\n",
    "    # Function to count unique elements of a series\n",
    "    return len(np.unique(series))\n",
    "\n",
    "def book_preprocessor(file_path):\n",
    "    # Function to preprocess book data (for each stock id)\n",
    "    \n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    \n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    \n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    \n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread':[np.sum, np.mean, np.std],\n",
    "        'price_spread2':[np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std],\n",
    "        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n",
    "    }\n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        # Function to get group stats for different windows (seconds in bucket)\n",
    "        \n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        \n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def trade_preprocessor(file_path):\n",
    "    # Function to preprocess trade data (for each stock id)\n",
    "    \n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n",
    "        'order_count':[np.mean,np.sum,np.max],\n",
    "    }\n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        # Function to get group stats for different windows (seconds in bucket)\n",
    "        \n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        \n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    \n",
    "    def order_sum(df, sec:str):\n",
    "        new_col = 'size_tau' + sec\n",
    "        bucket_col = 'trade_seconds_in_bucket_count_unique' + sec\n",
    "        df[new_col] = np.sqrt(1/df[bucket_col])\n",
    "        \n",
    "        new_col2 = 'size_tau2' + sec\n",
    "        order_col = 'trade_order_count_sum' + sec\n",
    "        df[new_col2] = np.sqrt(1/df[order_col])\n",
    "        \n",
    "        if sec == '400_':\n",
    "            df['size_tau2_d'] = df['size_tau2_400'] - df['size_tau2']\n",
    "        \n",
    "\n",
    "    \n",
    "    for sec in ['','_200','_300','_400']:\n",
    "        order_sum(df_feature, sec)\n",
    "        \n",
    "    df_feature['size_tau2_d'] = df_feature['size_tau2_400'] - df_feature['size_tau2']\n",
    "    \n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def get_time_stock(df):\n",
    "    # Function to get group stats for the stock_id and time_id\n",
    "    \n",
    "    # Get realized volatility columns\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_agg_features(train, test):\n",
    "\n",
    "    # Making agg features\n",
    "\n",
    "    train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "    corr = train_p.corr()\n",
    "    ids = corr.index\n",
    "    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "    l = []\n",
    "    for n in range(7):\n",
    "        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "\n",
    "    mat = []\n",
    "    matTest = []\n",
    "    n = 0\n",
    "    for ind in l:\n",
    "        newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "        newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "        mat.append ( newDf )\n",
    "        newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "        newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "        matTest.append ( newDf )\n",
    "        n+=1\n",
    "\n",
    "    mat1 = pd.concat(mat).reset_index()\n",
    "    mat1.drop(columns=['target'],inplace=True)\n",
    "    mat2 = pd.concat(matTest).reset_index()\n",
    "    \n",
    "    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "    \n",
    "    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "    mat1.reset_index(inplace=True)\n",
    "    \n",
    "    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "    mat2.reset_index(inplace=True)\n",
    "    \n",
    "    prefix = ['log_return1_realized_volatility', 'total_volume_mean', 'trade_size_mean', 'trade_order_count_mean','price_spread_mean','bid_spread_mean','ask_spread_mean',\n",
    "              'volume_imbalance_mean', 'bid_ask_spread_mean','size_tau2']\n",
    "    selected_cols=mat1.filter(regex='|'.join(f'^{x}.(0|1|3|4|6)c1' for x in prefix)).columns.tolist()\n",
    "    selected_cols.append('time_id')\n",
    "    \n",
    "    train_m = pd.merge(train,mat1[selected_cols],how='left',on='time_id')\n",
    "    test_m = pd.merge(test,mat2[selected_cols],how='left',on='time_id')\n",
    "    \n",
    "    # filling missing values with train means\n",
    "\n",
    "    features = [col for col in train_m.columns.tolist() if col not in ['time_id','target','row_id']]\n",
    "    train_m[features] = train_m[features].fillna(train_m[features].mean())\n",
    "    test_m[features] = test_m[features].fillna(train_m[features].mean())\n",
    "\n",
    "    return train_m, test_m\n",
    "    \n",
    "    \n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    # Funtion to make preprocessing function in parallel (for each stock id)\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    \n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516dc9a8",
   "metadata": {
    "papermill": {
     "duration": 0.017251,
     "end_time": "2025-04-19T09:13:34.672536",
     "exception": false,
     "start_time": "2025-04-19T09:13:34.655285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loding the and doing some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e86e7943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:13:34.713064Z",
     "iopub.status.busy": "2025-04-19T09:13:34.712575Z",
     "iopub.status.idle": "2025-04-19T09:39:32.783593Z",
     "shell.execute_reply": "2025-04-19T09:39:32.784000Z",
     "shell.execute_reply.started": "2025-04-13T18:11:53.417390Z"
    },
    "papermill": {
     "duration": 1558.094538,
     "end_time": "2025-04-19T09:39:32.784183",
     "exception": false,
     "start_time": "2025-04-19T09:13:34.689645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 22.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n",
    "\n",
    "# Fill inf values\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "\n",
    "# Aggregating some features\n",
    "train, test = create_agg_features(train,test)\n",
    "\n",
    "train.to_csv(\"processed_train.csv\", index=False)\n",
    "test.to_csv(\"processed_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c003e216",
   "metadata": {
    "papermill": {
     "duration": 0.017836,
     "end_time": "2025-04-19T09:39:32.821096",
     "exception": false,
     "start_time": "2025-04-19T09:39:32.803260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training\n",
    "\n",
    "First we selecting columns for the training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2a4e3b",
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T05:55:25.489Z",
     "iopub.execute_input": "2025-04-19T09:39:32.955747Z",
     "iopub.status.busy": "2025-04-19T09:39:32.954508Z",
     "iopub.status.idle": "2025-04-19T09:39:33.138192Z",
     "shell.execute_reply": "2025-04-19T09:39:33.137141Z"
    },
    "papermill": {
     "duration": 0.299228,
     "end_time": "2025-04-19T09:39:33.138335",
     "exception": false,
     "start_time": "2025-04-19T09:39:32.839107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
    "y = train['target']\n",
    "X_test=test.copy()\n",
    "X_test.drop(['time_id','row_id'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d657f",
   "metadata": {
    "papermill": {
     "duration": 0.021645,
     "end_time": "2025-04-19T09:39:33.193094",
     "exception": false,
     "start_time": "2025-04-19T09:39:33.171449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setting Loss and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5fbd9d5",
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T05:55:25.489Z",
     "iopub.execute_input": "2025-04-19T09:39:33.240299Z",
     "iopub.status.busy": "2025-04-19T09:39:33.239791Z",
     "iopub.status.idle": "2025-04-19T09:39:33.242344Z",
     "shell.execute_reply": "2025-04-19T09:39:33.241843Z"
    },
    "papermill": {
     "duration": 0.027639,
     "end_time": "2025-04-19T09:39:33.242450",
     "exception": false,
     "start_time": "2025-04-19T09:39:33.214811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RMSPE for offline validation\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "# MAE\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# R² Score\n",
    "def r2(y_true, y_pred):\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "# TabNet metric class for RMSPE\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "\n",
    "# PyTorch loss function for training\n",
    "def RMSPELoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean(((y_true - y_pred) / y_true) ** 2)).clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec42e7",
   "metadata": {
    "papermill": {
     "duration": 0.018002,
     "end_time": "2025-04-19T09:39:33.278609",
     "exception": false,
     "start_time": "2025-04-19T09:39:33.260607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### set categorical data and scale the numerical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28653f36",
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T05:55:25.489Z",
     "iopub.execute_input": "2025-04-19T09:39:33.320566Z",
     "iopub.status.busy": "2025-04-19T09:39:33.320013Z",
     "iopub.status.idle": "2025-04-19T09:39:41.768664Z",
     "shell.execute_reply": "2025-04-19T09:39:41.769090Z"
    },
    "papermill": {
     "duration": 8.472607,
     "end_time": "2025-04-19T09:39:41.769256",
     "exception": false,
     "start_time": "2025-04-19T09:39:33.296649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nunique = X.nunique()\n",
    "types = X.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "\n",
    "for col in X.columns:\n",
    "    if  col == 'stock_id':\n",
    "        l_enc = LabelEncoder()\n",
    "        X[col] = l_enc.fit_transform(X[col].values)\n",
    "        X_test[col] = l_enc.transform(X_test[col].values)\n",
    "        categorical_columns.append(col)\n",
    "        categorical_dims[col] = len(l_enc.classes_)\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        X[col] = scaler.fit_transform(X[col].values.reshape(-1, 1))\n",
    "        X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n",
    "        \n",
    "\n",
    "\n",
    "cat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n",
    "\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 2344753,
     "sourceId": 27233,
     "sourceType": "competition"
    },
    {
     "datasetId": 1554453,
     "sourceId": 2561708,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30121,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21362.471693,
   "end_time": "2025-04-19T15:08:48.568452",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-19T09:12:46.096759",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
